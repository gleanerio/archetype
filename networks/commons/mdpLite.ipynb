{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROt0OdUHCQjA"
      },
      "source": [
        "# OIH Master Data Product Generator\n",
        "\n",
        "## Product concepts\n",
        "\n",
        "* solr\n",
        "* WMO WIS2\n",
        "* state of the ocean report (shacl + s2/h3 needed for this too)\n",
        "* Duplication checks  (or do in the graph directly)\n",
        "\n",
        "\n",
        "## About\n",
        "This notebook is a test some approaches for processing the release graphs into a format that is useful for the Solr index\n",
        "\n",
        "## ADRs\n",
        "\n",
        "1)\n",
        "\n",
        "\n",
        "\n",
        "## Look at\n",
        "\n",
        "* https://github.com/sybrenjansen/mpire\n",
        "\n",
        "## Example Solr input at\n",
        "\n",
        "https://github.com/iodepo/odis-arch/blob/master/graphOps/extraction/solr/solrexample.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjUetV1istXH"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBiniwAmCQjB"
      },
      "source": [
        "## requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpmar4p1Cerc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q rdflib\n",
        "!pip install -q shapely\n",
        "!pip install -q pyld\n",
        "!pip install -q kglab\n",
        "!pip install -q minio\n",
        "!pip install -q objdict\n",
        "!pip install -q shapely\n",
        "!pip install -q geopandas\n",
        "!pip install -q oxrdflib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DSr8qjAcsKs"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdz1-QBbCQjC"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)  ## remove pandas future warning\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "# import s3fs\n",
        "import pyarrow.parquet as pq\n",
        "import shapely\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import json, io\n",
        "from pyld import jsonld\n",
        "import kglab\n",
        "from minio import Minio\n",
        "import rdflib\n",
        "from rdflib import ConjunctiveGraph  #  needed for nquads\n",
        "from urllib.request import urlopen\n",
        "from dateutil import parser\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from pyproj import Geod\n",
        "import oxrdflib # https://github.com/oxigraph/oxrdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnRmTvEiCQjD"
      },
      "outputs": [],
      "source": [
        "# Check for using GPU, in case you want to ensure your GPU is used\n",
        "# gc = kglab.get_gpu_count()\n",
        "# print(gc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhnZ6kQM-1w3"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWmEYoqBwx6K"
      },
      "outputs": [],
      "source": [
        "# pop out last element in a quad to make a triple\n",
        "def popper(input):\n",
        "    lines = input.decode().split('\\n') # Split input into separate lines\n",
        "    modified_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        newline = line.replace(\"http://schema.org\", \"https://schema.org\")\n",
        "        segments = newline.split(' ')\n",
        "\n",
        "        if len(segments) > 3:\n",
        "            segments.pop()   # Remove the last two segment\n",
        "            segments.pop()\n",
        "            new_line = ' '.join(segments) + ' .'\n",
        "            modified_lines.append(new_line)\n",
        "\n",
        "    result_string = '\\n'.join(modified_lines)\n",
        "\n",
        "    return(result_string)\n",
        "\n",
        "def contextAlignment(input):\n",
        "    lines = input.decode().split('\\n') # Split input into separate lines\n",
        "    modified_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        newline = line.replace(\"http://schema.org\", \"https://schema.org\")\n",
        "\n",
        "        modified_lines.append(newline)\n",
        "\n",
        "    result_string = '\\n'.join(modified_lines)\n",
        "\n",
        "    return(result_string)\n",
        "\n",
        "def publicurls(client, bucket, prefix):\n",
        "    urls = []\n",
        "    objects = client.list_objects(bucket, prefix=prefix, recursive=True)\n",
        "    for obj in objects:\n",
        "        result = client.stat_object(bucket, obj.object_name)\n",
        "\n",
        "        if result.size > 0:  #  how to tell if an objet   obj.is_public  ?????\n",
        "            url = client.presigned_get_object(bucket, obj.object_name)\n",
        "            # print(f\"Public URL for object: {url}\")\n",
        "            urls.append(url)\n",
        "\n",
        "    return urls\n",
        "\n",
        "def to_wkt(polygon_string):\n",
        "    # split the input string into pairs\n",
        "    pairs = polygon_string.split(',')\n",
        "\n",
        "    # transform each pair into 'y x' format\n",
        "    # transformed_pairs = [' '.join(reversed(pair.split())) for pair in pairs]\n",
        "    transformed_pairs = [' '.join(pair.split()) for pair in pairs]\n",
        "\n",
        "\n",
        "    # join the transformed pairs with a comma and a space\n",
        "    transformed_string = ', '.join(transformed_pairs)\n",
        "\n",
        "    # return the final WKT string\n",
        "    return f\"POLYGON (({transformed_string}))\"\n",
        "\n",
        "def contains_alpha(s):\n",
        "    if isinstance(s, (int, float)):\n",
        "      return False\n",
        "    return any(c.isalpha() for c in s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbs89hRsCQjF"
      },
      "source": [
        "\n",
        "## Load Graph(s)\n",
        "\n",
        "At this point we have the URLs, and we could either loop load all of them or pull one out manually and use.  This section dmonstrates loading and working with one\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSvg2-PACQjE",
        "outputId": "f13a928e-aa36-4605-bb65-53a7cc3ee85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://ossapi.oceaninfohub.org/public/graphs/summonedafricaioc_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedaquadocs_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedcioos_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonededmerp_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonededmo_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedemodnet_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinanodc_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinvemardocuments_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinvemarexperts_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinvemarinstitutions_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinvemartraining_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedinvemarvessels_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedmarinetraining_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedobis_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedobps_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedoceanexperts_v1_release.nq\n",
            "http://ossapi.oceaninfohub.org/public/graphs/summonedpdh_v1_release.nq\n"
          ]
        }
      ],
      "source": [
        "client = Minio(\"ossapi.oceaninfohub.org:80\",  secure=False) # Create client with anonymous access.\n",
        "urls = publicurls(client, \"public\", \"graph\")\n",
        "for u in urls:\n",
        "  print(u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbUIqG10CQjG",
        "outputId": "8a402c67-35fe-4266-ad1a-cdbf31d79e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "901711\n"
          ]
        }
      ],
      "source": [
        "# load single quad graph into a RDFLIB conjunctive graph\n",
        "\n",
        "# u = \"http://ossapi.oceaninfohub.org/public/graphs/summonedcioos_v1_release.nq\"\n",
        "u = \"http://ossapi.oceaninfohub.org/public/graphs/summonedoceanexperts_v1_release.nq\"\n",
        "\n",
        "df = urlopen(u)\n",
        "dg = df.read()\n",
        "r = contextAlignment(dg)\n",
        "\n",
        "g = ConjunctiveGraph()\n",
        "# g = rdflib.ConjunctiveGraph(store=\"Oxigraph\")\n",
        "g.parse(data=r, format=\"nquads\")\n",
        "print(len(g))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOKJIkmmQJkB"
      },
      "outputs": [],
      "source": [
        "# # load all graphs\n",
        "\n",
        "# g = ConjunctiveGraph()\n",
        "# for u in urls:\n",
        "#   print(\"loading: {}\".format(u))\n",
        "\n",
        "#   df = urlopen(u)\n",
        "#   dg = df.read()\n",
        "#   r = contextAlignment(dg)\n",
        "\n",
        "#   g.parse(data=r, format=\"nquads\")\n",
        "\n",
        "# print(len(g))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrcfOITdCQjG"
      },
      "outputs": [],
      "source": [
        "## Convert the RDFLIB graph to a kglabs graph\n",
        "\n",
        "namespaces = {\n",
        "    \"sh\":   \"http://www.w3.org/ns/shacl#\" ,\n",
        "    \"schema\":   \"https://schema.org/\" ,\n",
        "    \"geo\":      \"http://www.opengis.net/ont/geosparql#\",\n",
        "}\n",
        "\n",
        "kg = kglab.KnowledgeGraph(name = \"OIH test\", base_uri = \"https://oceaninfohub.org/id/\", namespaces = namespaces, use_gpus=True, import_graph = g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEEcv606dro0"
      },
      "source": [
        "## Query Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7EVoF89cxWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List of URLs\n",
        "#     \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/baseQuery.rq\",\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/baseQuery.rq\",\n",
        "    \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/sup_geo.rq\",\n",
        "    \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/address_geo.rq\",\n",
        "    \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/sup_temporal.rq\",\n",
        "    \"https://raw.githubusercontent.com/iodepo/odis-in/master/SPARQL/searchOIH/dataset.rq\"\n",
        "]\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            # Extract the file name from the URL and change \".rq\" to \"rq\"\n",
        "            file_name = url.split(\"/\")[-1].replace(\".rq\", \"rq\")\n",
        "            content = response.text\n",
        "\n",
        "            # Create a variable with the modified name and store the content\n",
        "            globals()[file_name] = content\n",
        "        else:\n",
        "            print(f\"Failed to download URL {url}. Status code: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTbmGG9HeuXo",
        "outputId": "e0113455-9c8d-41e9-f935-dc860911c62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
            "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
            "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
            "PREFIX schema: <https://schema.org/>\n",
            "PREFIX geosparql: <http://www.opengis.net/ont/geosparql#>\n",
            "\n",
            "SELECT (?s as ?id) ?type ?address ?addressCountry ?country\n",
            "WHERE {\n",
            "    ?s rdf:type ?type .\n",
            "    FILTER (?type IN (schema:ResearchProject, schema:Project, schema:Organization,\n",
            "                         schema:Dataset, schema:CreativeWork, schema:Person, schema:Map, schema:Course,\n",
            "                         schema:CourseInstance, schema:Event, schema:Vehicle )\n",
            "    )\n",
            "    FILTER (isIRI(?s))\n",
            "    OPTIONAL {\n",
            "        ?s schema:nationality ?nat .\n",
            "        ?nat a schema:Country .\n",
            "        ?nat schema:name ?country .\n",
            "    }\n",
            "    OPTIONAL {\n",
            "        ?s schema:spatialCoverage ?sc .\n",
            "        ?sc a schema:Place .\n",
            "        ?sc schema:geo ?geo .\n",
            "        OPTIONAL {\n",
            "            ?geo schema:address ?address .\n",
            "        }\n",
            "        OPTIONAL {\n",
            "            ?geo schema:addressCountry ?addressCountry .\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(address_georq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVcyovDqDGdm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # this expects -- spatialCoverage -> Place -- geo --> GeoShape\n",
        "# # could prefix asWKT with hasGeometry\n",
        "# geoq = \"\"\"\n",
        "# PREFIX schema: <https://schema.org/>\n",
        "# PREFIX geosparql: <http://www.opengis.net/ont/geosparql#>\n",
        "\n",
        "# SELECT (?s as ?id) ?type ?name ?geotype ?geompred ?geom ?lat ?long\n",
        "# WHERE\n",
        "# {\n",
        "#     ?s rdf:type ?type .\n",
        "#     FILTER ( ?type IN (schema:ResearchProject, schema:Project, schema:Organization,\n",
        "#     schema:Dataset, schema:CreativeWork, schema:Person, schema:Map, schema:Course,\n",
        "#     schema:CourseInstance, schema:Event, schema:Vehicle ) )\n",
        "#     ?s schema:spatialCoverage ?sc .\n",
        "#     ?sc a schema:Place .\n",
        "#     OPTIONAL { ?sc schema:name ?name } .\n",
        "#     OPTIONAL {\n",
        "#       ?sc schema:latitude ?lat .\n",
        "#       ?sc schema:longitude ?long .\n",
        "#     }\n",
        "#     OPTIONAL {\n",
        "#       ?sc schema:geo ?geo .\n",
        "#       ?geo a ?geotype .\n",
        "#       ?geo ?geompred ?geom .\n",
        "#       FILTER(!isIRI(?geom))\n",
        "#     }\n",
        "#     OPTIONAL {\n",
        "#       ?gs geosparql:asWKT ?wkt\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# timeq = \"\"\"\n",
        "# PREFIX schema: <https://schema.org/>\n",
        "\n",
        "# SELECT (?s as ?id) ?type ?time ?temporalCoverage ?dateModified ?datePublished\n",
        "# WHERE\n",
        "# {\n",
        "#     ?s rdf:type ?type .\n",
        "#     FILTER ( ?type IN (schema:ResearchProject, schema:Project, schema:Organization,\n",
        "#     schema:Dataset, schema:CreativeWork, schema:Person, schema:Map, schema:Course,\n",
        "#     schema:CourseInstance, schema:Event, schema:Vehicle ) )\n",
        "\n",
        "#     OPTIONAL { ?s schema:temporalCoverage ?temporalCoverage }\n",
        "#     OPTIONAL { ?s schema:dataModified   ?dataModified }\n",
        "#     OPTIONAL { ?s schema:datePublished   ?datePublished }\n",
        "\n",
        "\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# datasetq = \"\"\"\n",
        "# PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "# PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
        "# prefix prov: <http://www.w3.org/ns/prov#>\n",
        "# PREFIX schema: <https://schema.org/>\n",
        "\n",
        "# SELECT DISTINCT (?s as ?id) ?type ?name ?headline ?url ?description ?sameAs ?license ?citation ?keywords ?includedInDataCatalog ?distribution ?region ?provider ?publisher ?creator\n",
        "# WHERE {\n",
        "#     graph ?g {\n",
        "#         BIND(schema:Dataset AS ?type)\n",
        "#         ?s rdf:type ?type .\n",
        "\n",
        "#         OPTIONAL { ?s schema:name ?name . }\n",
        "#         OPTIONAL { ?s schema:headline ?headline . }\n",
        "#         OPTIONAL { ?s schema:url ?url . }\n",
        "#         OPTIONAL { ?s schema:description ?description . }\n",
        "\n",
        "#         OPTIONAL { ?s schema:sameAs ?sameAs . }\n",
        "#         OPTIONAL { ?s schema:license ?license . }\n",
        "#         OPTIONAL { ?s schema:citation ?citation . }\n",
        "#         OPTIONAL { ?s schema:keywords ?keywords . }\n",
        "#         OPTIONAL { ?s schema:includedInDataCatalog ?includedInDataCatalog . }\n",
        "#         OPTIONAL { ?s schema:distribution ?distribution . }\n",
        "#         OPTIONAL { ?s schema:region ?region . }\n",
        "#         OPTIONAL { ?s schema:provider ?provider . }\n",
        "#         OPTIONAL { ?s schema:publisher ?publisher .}\n",
        "#         OPTIONAL { ?s schema:creator ?creator . }\n",
        "#         }\n",
        "# }\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eR7AtmLSqI"
      },
      "source": [
        "## Notes\n",
        "\n",
        "* many of the above can be arrays, we need to note this in the shacl cardinality\n",
        "* also the Pandas dataframes need to roll these up into comma seperated items, ie, python lists via aggregate and join.   These will then serialize out to solr JSON correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYHCW9rsvHE9"
      },
      "outputs": [],
      "source": [
        "##would be nice to do a type count SPARQL here as a sanity check...\n",
        "# counts = \"\"\"\n",
        "# PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "\n",
        "\n",
        "# SELECT  ( COUNT( DISTINCT ?s) as ?count) ?type\n",
        "# WHERE\n",
        "#  {\n",
        "#      graph ?g {\n",
        "#          ?s rdf:type ?type .\n",
        "#      }\n",
        "# }\n",
        "# GROUP BY ?type\n",
        "# ORDER BY DESC(?count)\n",
        "# \"\"\"\n",
        "\n",
        "# cdf = kg.query_as_df(counts)\n",
        "\n",
        "# cdf.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPphnXlUYe1"
      },
      "source": [
        "## Loop on Queries\n",
        "\n",
        "NOTE, do the queries need isIRI for the subject to avoid resoruces without a top level ID?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9XcFOhLUX_n",
        "outputId": "86eaabd4-5c78-4191-88cc-c874940235cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "209367\n",
            "0\n",
            "0\n",
            "50944\n"
          ]
        }
      ],
      "source": [
        "qlist = [baseQueryrq, datasetrq,  sup_georq, sup_temporalrq, address_georq]\n",
        "\n",
        "# m1 = pd.merge(pdf, geodf, on='id', how='outer')\n",
        "# mf = pd.DataFrame()\n",
        "dfl = []\n",
        "for q in qlist:\n",
        "  df = kg.query_as_df(q)\n",
        "  print(len(df))\n",
        "  if len(df) > 0:\n",
        "    dfl.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhIASbwN8hF3"
      },
      "outputs": [],
      "source": [
        "common_column = [\"id\", \"type\"]  # Replace with the actual common column name\n",
        "\n",
        "# Initialize a merged DataFrame with the first DataFrame\n",
        "merged_df = dfl[0]\n",
        "\n",
        "# Iterate through the remaining DataFrames and merge them into the merged_df\n",
        "for df in dfl[1:]:\n",
        "    merged_df = pd.merge(merged_df, df, on=common_column, how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RglRhs44rXjt",
        "outputId": "81e2b9e7-6ef4-469b-b35d-1020948c3cad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50944"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df['id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB6fIOW_bOSL",
        "outputId": "ebdd4946-bbbe-4d0f-f8e7-1d36e1037e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67447 entries, 0 to 67446\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   id           67447 non-null  object\n",
            " 1   type         67447 non-null  object\n",
            " 2   name         49112 non-null  object\n",
            " 3   url          59578 non-null  object\n",
            " 4   description  22507 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 2.6+ MB\n"
          ]
        }
      ],
      "source": [
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJpmnVCGzamw"
      },
      "source": [
        "## Post processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJbUhtXp4t8a"
      },
      "source": [
        "### GeoSpatial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi3173QJzeCV"
      },
      "outputs": [],
      "source": [
        "merged_df['filteredgeom'] = merged_df['geom'].apply(lambda x: np.nan if contains_alpha(x) else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCvbGG7o5HJy"
      },
      "source": [
        "### Regions\n",
        "Incorporate Jeff's regions.py which needs\n",
        "\n",
        "* address (Org, person, Course?\n",
        "* name (THING, in all)\n",
        "* spatialFeature (WKT geom column)\n",
        "* countryOfLastProcessing (vehicle only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVbZ1sheLbn2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def normalize(s):\n",
        "    if isinstance(s, str):\n",
        "      s = s.lower()\n",
        "      s = re.sub(r\"\\(.*\\)\",\"\",s)\n",
        "      s = re.sub(r\"\\[.*\\]\",\"\",s)\n",
        "      s = re.sub(r\"and|the|of\",\"\", s)\n",
        "      s = s.rstrip('.')\n",
        "      return set(s.split(None))\n",
        "    else:\n",
        "      return set(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "TT3LHNQOKMC-",
        "outputId": "b64b39f3-8d25-4a5f-80f1-cf37a2ead07c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b3959b148c9f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/regions-clipped.geojson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mgeo_regions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeo_regions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/regions-clipped.geojson'"
          ]
        }
      ],
      "source": [
        "# from . import datashaping\n",
        "import shapely.geometry\n",
        "import os, json\n",
        "\n",
        "import shapely.wkt\n",
        "import shapely.geometry\n",
        "from urllib.request import urlopen\n",
        "\n",
        "with open('/content/regions-clipped.geojson', 'r') as f:\n",
        "    geo_regions = json.load(f)['features']\n",
        "    for r in geo_regions:\n",
        "        r['shape'] = shapely.geometry.shape(r['geometry'])\n",
        "\n",
        "# leverage the UNSD API \"GeoArea\" JSON endpoint, instead of locally-stored CSV\n",
        "#  see https://unstats.un.org/SDGAPI/swagger/\n",
        "unsdGeoareaEndpoint = \"https://unstats.un.org/SDGAPI/v1/sdg/GeoArea/Tree\"\n",
        "response = urlopen(unsdGeoareaEndpoint)\n",
        "unsdDataJSON = json.loads(response.read())\n",
        "\n",
        "# use the \"World (total) by continental regions\" branch\n",
        "continentalRegions = unsdDataJSON[1]\n",
        "continentalRegionsChildren = continentalRegions['children']\n",
        "\n",
        "# parse the JSON from the API call\n",
        "countries_dict_with_regions = {}\n",
        "country_map_list = []\n",
        "\n",
        "for list_regions in continentalRegionsChildren:\n",
        "    if list_regions['children'] == None:\n",
        "        regionName = list_regions['geoAreaName']\n",
        "        # print('Region name (no children): ' + regionName)\n",
        "    else:\n",
        "        regionName = list_regions['geoAreaName']\n",
        "        # print('Region name: ' + regionName)\n",
        "        # loop through sub-regions\n",
        "        for list_subregions in list_regions['children']:\n",
        "            subRegionName = list_subregions['geoAreaName']\n",
        "            # print('Sub-region name: ' + subRegionName)\n",
        "            # loop through intermediate region items\n",
        "            for list_intermediate_regions in list_subregions['children']:\n",
        "                if list_intermediate_regions['type'] == 'Region':\n",
        "                    intermediateRegionName = list_intermediate_regions['geoAreaName']\n",
        "                    # print('Intermediate region name: ' + intermediateRegionName)\n",
        "                    # loop through intermediate region children\n",
        "                    for list_intermediate_region_children in list_intermediate_regions['children']:\n",
        "                        countryName = list_intermediate_region_children['geoAreaName'].lower()\n",
        "                        # print('Country name: ' + countryName)\n",
        "                        countries_dict_with_regions[countryName] = [regionName, subRegionName]\n",
        "                        country_map_list.append((normalize(countryName), countryName))\n",
        "                else:\n",
        "                    countryName = list_intermediate_regions['geoAreaName'].lower()\n",
        "                    # print('Country name: ' + countryName)\n",
        "                    countries_dict_with_regions[countryName] = [regionName, subRegionName]\n",
        "                    country_map_list.append((normalize(countryName), countryName))\n",
        "\n",
        "\n",
        "def address(address):\n",
        "    normalized = normalize(address)\n",
        "    value = list()\n",
        "    for parts, country in country_map_list:\n",
        "        if parts <= normalized:\n",
        "          if country in countries_dict_with_regions:\n",
        "            value = countries_dict_with_regions[country]\n",
        "    return value\n",
        "\n",
        "def name(n):\n",
        "    normalized = normalize(n)\n",
        "    value = list()\n",
        "    for parts, country in country_map_list:\n",
        "      if parts <= normalized:\n",
        "        if country in countries_dict_with_regions:\n",
        "          value = countries_dict_with_regions[country]\n",
        "    return value\n",
        "\n",
        "\n",
        "def countryLastProcessing(countryOfLastProcessing):\n",
        "    normalized = normalize(countryOfLastProcessing)\n",
        "    value = list()\n",
        "    for parts, country in country_map_list:\n",
        "        if parts <= normalized:\n",
        "          if country in countries_dict_with_regions:\n",
        "            value = countries_dict_with_regions[country]\n",
        "    return value\n",
        "\n",
        "\n",
        "def feature(feature):\n",
        "    try:\n",
        "        the_geom = shapely.wkt.loads(feature)\n",
        "        return [r['properties']['name'] for r in geo_regions if r['shape'].intersects(the_geom)]\n",
        "    except:\n",
        "        print(\"Invalid WKT\")\n",
        "        return list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6k7-XQ_fLOF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data1 = [\"Marine Science Country Profiles : Kenya\",\n",
        "        \"The fisheries of Barbados and some of their problems\",\n",
        "        \"Fiji : Where's the data?\", 'POLYGON ((-95.5 19.5,-95.5 31.5,-73.5 31.5,-73.5 19.5,-95.5 19.5))']\n",
        "\n",
        "data2 = [\"Marine Science Country Profiles : Kenya\",\n",
        "        \"The fisheries of Barbados and some of their problems\",\n",
        "        \"Fiji : Where's the data?\"]\n",
        "\n",
        "dftest = pd.DataFrame({'name': data1})\n",
        "dftest.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eGk_NE8Uw0O"
      },
      "outputs": [],
      "source": [
        "# merged_df['region'] = merged_df['name'].apply(lambda x: x + [item for item in name(x) if item not in x] if x else x)\n",
        "dftest['nregion'] = dftest['name'].apply(lambda x: name(x)  if x else x)\n",
        "dftest['aregion'] = dftest['name'].apply(lambda x: address(x)  if x else x)\n",
        "dftest['cregion'] = dftest['name'].apply(lambda x: countryLastProcessing(x)  if x else x)\n",
        "dftest['fregion'] = dftest['name'].apply(lambda x: feature(x)  if x else x)\n",
        "\n",
        "dftest.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFQEfLr-xKVi"
      },
      "outputs": [],
      "source": [
        "def g(df):\n",
        "    df['region'] = df[['nregion', 'aregion','cregion', 'fregion']].apply(lambda x: list(set(x[0] + x[1] + x[2]+ x[3])), axis=1)\n",
        "    del df['nregion']\n",
        "    del df['aregion']\n",
        "    del df['cregion']\n",
        "    del df['fregion']\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVqOudG4xfk4"
      },
      "outputs": [],
      "source": [
        "dftest = g(dftest.copy())\n",
        "\n",
        "dftest.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7igZku159VNZ"
      },
      "outputs": [],
      "source": [
        "def make_pairs(ll):\n",
        "  \"\"\"Makes pairs of coordinates from a list of coordinates.\n",
        "\n",
        "  Args:\n",
        "    ll: A list of coordinates.\n",
        "\n",
        "  Returns:\n",
        "    A list of pairs of coordinates.\n",
        "  \"\"\"\n",
        "\n",
        "  coords = []\n",
        "  for i in range(0, len(ll), 2):\n",
        "    coords.append((ll[i], ll[i+1]))\n",
        "\n",
        "  return coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96AMKBOGRngl"
      },
      "outputs": [],
      "source": [
        "def gj(geom, value):\n",
        "  test = geom.split()\n",
        "  test = [float(x) for x in test]\n",
        "  if len(test) < 2:\n",
        "    return None\n",
        "\n",
        "  cp = make_pairs(test)\n",
        "\n",
        "  if len(cp) == 1:\n",
        "    # print(\"POINT\")\n",
        "    geom = shapely.Point(cp)\n",
        "  elif len(cp) == 2:\n",
        "    # print(\"BOX\")\n",
        "    geom = shapely.box(cp[0][0], cp[0][1], cp[1][0], cp[1][1])\n",
        "  else:\n",
        "    # print(\"POLYGON\")\n",
        "    geom = shapely.Polygon(cp)\n",
        "\n",
        "  if value == \"centroid\":\n",
        "    return geom.centroid\n",
        "  elif value == \"length\":\n",
        "    return geom.length\n",
        "  elif value == \"area\":\n",
        "    geod = Geod(ellps=\"WGS84\")\n",
        "    area = abs(geod.geometry_area_perimeter(geom)[0])\n",
        "    return area\n",
        "  elif value == \"wkt\":\n",
        "    return shapely.to_wkt(geom)\n",
        "  elif value == \"geojson\":\n",
        "    return shapely.to_geojson(geom)\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vizQZ3imRwmQ"
      },
      "outputs": [],
      "source": [
        "# merged_df['dt_endDate'] = merged_df['temporalCoverage'].apply(lambda x: re.split(\"/\", x)[1] if \"/\" in x else np.nan)\n",
        "\n",
        "merged_df['centroid'] = merged_df['filteredgeom'].apply(lambda x: gj(str(x), \"centroid\"))\n",
        "merged_df['length'] = merged_df['filteredgeom'].apply(lambda x: gj(str(x), \"length\"))\n",
        "merged_df['area'] = merged_df['filteredgeom'].apply(lambda x: gj(str(x), \"area\"))\n",
        "merged_df['wkt'] = merged_df['filteredgeom'].apply(lambda x: gj(str(x), \"wkt\"))\n",
        "merged_df['geojson'] = merged_df['filteredgeom'].apply(lambda x: gj(str(x), \"geojson\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mp4_0kQ5MQr"
      },
      "outputs": [],
      "source": [
        "# merged_df[].head()\n",
        "print(merged_df[\"wkt\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxZrniVD4w5o"
      },
      "source": [
        "### Temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F8-w8pBzdxZ"
      },
      "outputs": [],
      "source": [
        "merged_df['temporalCoverage'] = merged_df['temporalCoverage'].astype('str')  # fine to make str since we don't use in the solr JSON\n",
        "merged_df['dt_startDate'] = merged_df['temporalCoverage'].apply(lambda x: re.split(\"/\", x)[0] if \"/\" in x else np.nan)\n",
        "merged_df['dt_endDate'] = merged_df['temporalCoverage'].apply(lambda x: re.split(\"/\", x)[1] if \"/\" in x else np.nan)\n",
        "merged_df['n_startYear'] = merged_df['dt_startDate'].apply(lambda x: parser.parse(x).year if \"-\" in str(x) else np.nan)\n",
        "merged_df['n_endYear'] = merged_df['dt_endDate'].apply(lambda x: parser.parse(x).year if \"-\" in str(x) else np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV2Z7Jq5PqBN"
      },
      "outputs": [],
      "source": [
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WvtX_xp0tjo"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b5cg711tVx"
      },
      "outputs": [],
      "source": [
        "# transforms needed for aggregation\n",
        "merged_df['keywords'] = merged_df['keywords'].astype(str)  #  why is this needed?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehVW6ZHuzeP0"
      },
      "outputs": [],
      "source": [
        "mf = merged_df.groupby('id').agg({'keywords': ', '.join,\n",
        "                                        'type': 'first',\n",
        "                                        'name': ', '.join,\n",
        "                                        'description': ', '.join,\n",
        "                                        'url': ', '.join,\n",
        "                                        'geotype':'first',\n",
        "                                        'geompred':'first',\n",
        "                                        'geom':'first',\n",
        "                                        'temporalCoverage': 'first',\n",
        "                                        'datePublished': 'first',\n",
        "                                        'license': 'first',\n",
        "                                        'creator': 'first',\n",
        "                                        'includedInDataCatalog': 'first',\n",
        "                                        'distribution': 'first',\n",
        "                                        'publisher': 'first',\n",
        "                                        'filteredgeom': 'first',\n",
        "                                        'dt_startDate': 'first',\n",
        "                                        'dt_endDate': 'first',\n",
        "                                        'n_startYear': 'first',\n",
        "                                        'n_endYear': 'first'}).reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxP9e2Cu11wB"
      },
      "outputs": [],
      "source": [
        "mf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCX3yX_FuHWQ"
      },
      "source": [
        "## Outpt JSON for Solr\n",
        "\n",
        "Example Records\n",
        "\n",
        "https://github.com/iodepo/odis-arch/blob/master/graphOps/extraction/solr/solrexample.json\n",
        "\n",
        "for\n",
        "\n",
        "https://catalogue.cioos.ca/dataset/ff0232d8-34bd-4456-be28-20d4f8b2937c.jsonld\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxjRZwaJCQjH"
      },
      "outputs": [],
      "source": [
        "mf.to_parquet('solr_set.parquet')\n",
        "mf.to_csv('solr_set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbX1VosTtk7q"
      },
      "outputs": [],
      "source": [
        "output_directory = 'json_output'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09PbNN5XuNZt"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOi3DjPA36Hq"
      },
      "outputs": [],
      "source": [
        "def remove_brackets(string):\n",
        "  if isinstance(string, (int, float)):\n",
        "    return string\n",
        "  if string.startswith('<') and string.endswith('>'):\n",
        "    return string[1:-1]\n",
        "  else:\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_3kp6neuOCX"
      },
      "outputs": [],
      "source": [
        "from objdict import ObjDict\n",
        "\n",
        "for index, row in mf.iterrows():\n",
        "    # Create a JSON string from the row\n",
        "    # json_string = row.to_json()\n",
        "\n",
        "    data = ObjDict()\n",
        "    sd = ObjDict()\n",
        "\n",
        "    # not in arrays\n",
        "    data.id = remove_brackets(row['id'])\n",
        "    data.type = row['type']\n",
        "\n",
        "    if not isinstance(row['keywords'] , (int, float)):\n",
        "      data.txt_keywords = [x.strip() for x in row['keywords'].split(',')]\n",
        "    if not isinstance(row['name'] , (int, float)):\n",
        "      data.txt_name = row['name']\n",
        "    data.description = row['description']\n",
        "    data.txt_url = [row['url']]\n",
        "    data.txt_license = [row['license']]\n",
        "    data.txt_creator = [row['creator']]\n",
        "    data.txt_includedInDataCatalog = [remove_brackets(row['includedInDataCatalog'])]\n",
        "    data.txt_distribution = [row['distribution']]\n",
        "    data.txt_publisher = [remove_brackets(row['publisher'])]\n",
        "\n",
        "    # # geo\n",
        "    # if row[\"filteredgeom\"] != np.nan:\n",
        "    #      data.geotype = [row['geompred']]\n",
        "    #      data.geom = [ row[\"filteredgeom\"]]\n",
        "        #  data.geojson_point = [ row[\"filteredgeom\"]]\n",
        "        #  data.geojson_simple = [ row[\"filteredgeom\"]]\n",
        "        #  data.geojson_geom = [ row[\"filteredgeom\"]]\n",
        "        #  data.geom_area = [ row[\"filteredgeom\"]]\n",
        "        #  data.geom_length = [ row[\"filteredgeom\"]]\n",
        "\n",
        "    # temporal\n",
        "    data.dt_startDate = [row['dt_startDate']]\n",
        "    data.dt_endDate = [row['dt_endDate']]\n",
        "    data.n_startYear = [row['n_startYear']]\n",
        "    data.n_endYear = [ row['n_endYear']]\n",
        "\n",
        "     # write\n",
        "    json_string = data.dumps(indent=4)\n",
        "\n",
        "    # Define the filename based on the row index or a unique identifier from your data\n",
        "    filename = os.path.join(output_directory, f'row_{index}.json')\n",
        "\n",
        "    # Write the JSON string to the file\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json_file.write(json_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr80MCtTv4wM"
      },
      "outputs": [],
      "source": [
        "### Core Items\n",
        "## takes around 3 minutes for core query\n",
        "# pdfstep1 = kg.query_as_df(coreq)\n",
        "# pdfstep1['keywords'] = pdfstep1['keywords'].astype(str)  #  why is this needed?\n",
        "# pdfstep1 = pdfstep1.astype(str)\n",
        "# pdfstep1.info()\n",
        "## need to role up the keywords into one cell\n",
        "# pdf = pdfstep1.groupby('id').agg({'keywords': ', '.join, 'type': 'first', 'name': ', '.join, 'description': ', '.join, 'url': ', '.join}).reset_index()\n",
        "# geodf_grouped = geodf.groupby('id').agg({'geotype': 'first', 'geompred': 'first', 'geom': 'first', 'filteredgeom': 'first'}).reset_index()\n",
        "### Spatial Sections\n",
        "\n",
        "# geodf = kg.query_as_df(geoq)\n",
        "## remove geojson entries detected via contains_alpha and turn them into \"\" or np.nan\n",
        "# geodf['filteredgeom'] = geodf['geom'].apply(lambda x: np.nan if contains_alpha(x) else x)\n",
        "### Temporal Sections\n",
        "## takes < 1 minute for temporal query\n",
        "# timedf = kg.query_as_df(timeq)\n",
        "# timedf['temporalCoverage'] = timedf['temporalCoverage'].astype('str')  # fine to make str since we don't use in the solr JSON\n",
        "# timedf['dt_startDate'] = timedf['temporalCoverage'].apply(lambda x: re.split(\"/\", x)[0] if \"/\" in x else np.nan)\n",
        "# timedf['dt_endDate'] = timedf['temporalCoverage'].apply(lambda x: re.split(\"/\", x)[1] if \"/\" in x else np.nan)\n",
        "# timedf['n_startYear'] = timedf['dt_startDate'].apply(lambda x: parser.parse(x).year if \"-\" in str(x) else np.nan)\n",
        "# timedf['n_endYear'] = timedf['dt_endDate'].apply(lambda x: parser.parse(x).year if \"-\" in str(x) else np.nan)\n",
        "# timedf.info()\n",
        "### Document section\n",
        "# docdf = kg.query_as_df(docq)\n",
        "# docdf.info()\n",
        "### Merge results together\n",
        "# mf = pd.merge(pdf, geodf, on='id', how='outer')\n",
        "# mf = pd.merge(mf, timedf, on='id', how='outer')\n",
        "# mf = pd.merge(mf, docdf, on='id', how='outer')\n",
        "## this was just an example to show we could remove rows with a specific NaN in a column\n",
        "# mf = mf.loc[mf['dt_startDate'].notnull()]\n",
        "# mf['id'].nunique()\n",
        "# mf.info()\n",
        "# mf.head(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}